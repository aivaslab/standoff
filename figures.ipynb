{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0af03d0d-3ff9-4c04-9051-af0d6e654159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments_utils import *\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2703227-2a1b-42f5-b4ea-45cd5d2d880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_comparison(combined_path_list, last_path_list, key_param_list, key_param, exp_name, params, prior_metrics, lp_list, used_regimes=None):\n",
    "\n",
    "    combined_path = os.path.join('supervised', exp_name, 'c')\n",
    "    os.makedirs(combined_path, exist_ok=True)\n",
    "\n",
    "    print('loading IFRs')\n",
    "    folder_paths = set()\n",
    "    for sublist in lp_list:\n",
    "        for file_path in sublist:\n",
    "            folder_path = os.path.dirname(file_path)\n",
    "            folder_paths.add(folder_path)\n",
    "\n",
    "    dataframes = []\n",
    "    for folder in folder_paths:\n",
    "        csv_path = os.path.join(folder, 'ifr_df.csv')\n",
    "        if os.path.exists(csv_path):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                base_name = os.path.basename(folder)\n",
    "                base_name = base_name.replace('-retrain', '')\n",
    "                df = df.dropna(subset=['epoch'])\n",
    "                #print('uniques', df['epoch'].unique())\n",
    "                df['prior'] = df['epoch'] == 0\n",
    "                df['model'] = base_name\n",
    "                df['retrain'] = 'retrain' in folder\n",
    "                dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {csv_path}: {e}\")\n",
    "\n",
    "    if len(dataframes) > 0:\n",
    "        combined_ifr_df = pd.concat(dataframes, ignore_index=True)\n",
    "        combined_ifr_df.to_csv(os.path.join(combined_path, 'combined_ifr_df.csv'), index=False)\n",
    "    columns_to_ignore = ['loss']\n",
    "    columns_to_consider = [col for col in combined_ifr_df.columns if col not in columns_to_ignore]\n",
    "    current_length = len(combined_ifr_df)\n",
    "    combined_ifr_df = combined_ifr_df.drop_duplicates(subset=columns_to_consider)\n",
    "    combined_ifr_df = adjust_epochs(combined_ifr_df)\n",
    "    print('duplicates removed', current_length, len(combined_ifr_df))\n",
    "\n",
    "    if False:\n",
    "        print('loading baseline h5s!!!!!')\n",
    "        all_baseline_data = []\n",
    "        base_path = os.path.dirname(list(folder_paths)[0])\n",
    "        for regime in ['s1', 's2', 's3']:\n",
    "            individual_data = load_h5_data(os.path.join(base_path, f'indy_ifr_base-{regime}-0.h5'), regime=regime, retrain=False, prior=False)\n",
    "            all_baseline_data.append(individual_data)\n",
    "        all_baseline_df = pd.concat(all_baseline_data, ignore_index=True)\n",
    "\n",
    "\n",
    "        ### Get individual data\n",
    "        print('loading model h5s')\n",
    "        all_indy_data = []\n",
    "        for folder in folder_paths:\n",
    "            regime = os.path.basename(folder).replace('-retrain', '')\n",
    "            retrain = 'retrain' in folder\n",
    "            prior = 'prior' in folder\n",
    "            for rep in [0, 1, 2]:\n",
    "                individual_data = load_h5_data(os.path.join(folder, f'indy_ifr{rep}.h5'), regime=regime, retrain=retrain, prior=prior)\n",
    "                all_indy_data.append(individual_data)\n",
    "        all_indy_df = pd.concat(all_indy_data, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "        if False:\n",
    "            all_indy_all = all_indy_df[all_indy_df['act_key'] == 'all-activations']\n",
    "            all_indy_final = all_indy_df[all_indy_df['act_key'] == 'final-layer-activations']\n",
    "            make_splom_aux(all_indy_all, 'all', os.path.join(combined_path, 'sploms'))\n",
    "            make_splom_aux(all_indy_final, 'final', os.path.join(combined_path, 'sploms'))\n",
    "            make_scatters2(all_indy_all, all_indy_final, other_keys, combined_path)\n",
    "\n",
    "        print('loading evaluation results')\n",
    "\n",
    "\n",
    "        all_epochs_df = load_dataframes(combined_path_list, key_param_list, key_param)\n",
    "        all_epochs_df['is_novel_task'] = all_epochs_df.apply(is_novel_task, axis=1)\n",
    "        print('columns and epochs', all_epochs_df.columns, all_epochs_df['epoch'].unique())\n",
    "\n",
    "        # run experiment 3:\n",
    "        if True:\n",
    "            print('running experiment 3')\n",
    "            print(all_epochs_df.columns)\n",
    "            print(all_indy_df.columns)\n",
    "            for retrain in [True, False]:\n",
    "                merged_df = pd.merge(all_indy_df[(all_indy_df['model_type'] == 'mlp1') & (all_indy_df['retrain'] == retrain)], all_epochs_df[['id', 'regime', 'is_novel_task', 'accuracy', 'repetition']], on=['id', 'regime', 'repetition'], how='left')\n",
    "                merged_df = merged_df.drop_duplicates()\n",
    "                print(len(merged_df))\n",
    "                print(merged_df.head())\n",
    "                print(merged_df.dtypes)\n",
    "                print(merged_df['accuracy'].describe())\n",
    "                print(merged_df['other_key'].value_counts())\n",
    "                do_prediction_dependency(merged_df, 'accuracy', combined_path, retrain)\n",
    "\n",
    "\n",
    "        all_epochs_df['regime_short'] = all_epochs_df['regime'].str[-2:]\n",
    "        merged_baselines = pd.merge(all_baseline_df, all_epochs_df[['id', 'regime_short', 'is_novel_task', 'retrain']], left_on=['id', 'regime', 'retrain'], right_on=['id', 'regime_short', 'retrain'], how='left')\n",
    "        all_baselines_df = merged_baselines\n",
    "        all_baselines_df = all_baselines_df.drop_duplicates()\n",
    "\n",
    "        # figure out whether each item in all_indy_df is novel:\n",
    "        merged_df = pd.merge(all_indy_df, all_epochs_df[['id', 'regime', 'is_novel_task', 'retrain']], on=['id', 'regime', 'retrain'], how='left')\n",
    "        all_indy_df = merged_df\n",
    "        all_indy_df = all_indy_df.drop_duplicates()\n",
    "\n",
    "        print('generating accuracy tables')\n",
    "        if False:\n",
    "            for retrain in [True, False]:\n",
    "                baseline_tables = generate_accuracy_tables(all_baselines_df[(all_baselines_df['retrain'] == retrain) & (all_baselines_df['prior'] == False)], combined_path, is_baseline=True, retrain=retrain)\n",
    "                result_tables = generate_accuracy_tables(all_indy_df[(all_indy_df['retrain'] == retrain) & (all_indy_df['prior'] == False)], combined_path, retrain=retrain)\n",
    "\n",
    "    # make pred dep heatmaps\n",
    "    if False:\n",
    "        print('doing dependency heatmaps')\n",
    "        for strat in ['normal', 'box']:\n",
    "            if strat == \"normal\":\n",
    "                strategies = {\n",
    "                    'No-Mindreading': ['opponents', 'big-loc', 'small-loc'],\n",
    "                    'Low-Mindreading': ['vision', 'fb-exist'],\n",
    "                    'High-Mindreading': ['fb-loc', 'b-loc', 'target-loc']\n",
    "                }\n",
    "            else:\n",
    "                strategies = {\n",
    "                    'No-Mindreading': ['big-loc', 'big-box', 'small-loc', 'small-box'],\n",
    "                    'High-Mindreading': ['fb-loc', 'fb-box', 'b-loc', 'b-box', 'target-loc', 'target-box',]\n",
    "                }\n",
    "\n",
    "\n",
    "            for retrain in [False, True]:\n",
    "                dep_df = pd.read_csv(os.path.join(combined_path, f'accuracy_dependencies_retrain_{retrain}.csv'))\n",
    "\n",
    "                for layer in ['all-activations', 'final-layer-activations']:\n",
    "                        plot_dependency_bar_graphs_new(dep_df, combined_path, strategies, True, retrain=retrain, strat=strat, layer=layer)\n",
    "            #create_faceted_heatmap(dep_df, True, 'final-layer-activations', os.path.join(combined_path, 'test.png'), strategies)\n",
    "\n",
    "    return combined_path\n",
    "    strategies_short = {\n",
    "        'Opponents': ['opponents'],\n",
    "        'Location Beliefs': ['b-loc']\n",
    "    }\n",
    "    strategies_long = {\n",
    "        'No-Mindreading': ['pred', 'opponents', 'big-loc', 'small-loc'],\n",
    "        'Low-Mindreading': ['vision', 'fb-exist'],\n",
    "        'High-Mindreading': ['fb-loc', 'b-loc', 'target-loc', 'labels']\n",
    "    }\n",
    "    strategies_both = {\n",
    "        'No-Mindreading': ['big-loc', 'big-box', 'small-loc', 'small-box'],\n",
    "        'High-Mindreading': ['fb-loc', 'fb-box', 'b-loc', 'b-box', 'target-loc', 'target-box', ]\n",
    "    }\n",
    "    for retrain in [True, False]:\n",
    "        baseline_tables = pd.read_csv(os.path.join(combined_path, f'base_all_table_retrain_False.csv'))\n",
    "        result_tables = pd.read_csv(os.path.join(combined_path, f'all_table_retrain_{retrain}.csv'))\n",
    "\n",
    "        print('made acc tables', len(baseline_tables), len(result_tables))\n",
    "        print(baseline_tables.columns, result_tables.columns)\n",
    "\n",
    "        this_path = os.path.join(combined_path, f'strats-rt-{retrain}')\n",
    "\n",
    "        for result_type in ['mlp1', 'linear']:\n",
    "            for layer in ['all', 'final-layer']:\n",
    "                plot_bar_graphs_new2(baseline_tables[(baseline_tables['Model_Type'] == result_type)], result_tables[(result_tables['Model_Type'] == result_type)], this_path, strategies_short, layer=layer, r_type=f'spec-{result_type}')\n",
    "                plot_bar_graphs_new2(baseline_tables[(baseline_tables['Model_Type'] == result_type)], result_tables[(result_tables['Model_Type'] == result_type)], this_path, strategies_long, layer=layer, r_type=result_type)\n",
    "                #plot_bar_graphs_new(baseline_tables[(baseline_tables['Model_Type'] == result_type)], result_tables[(result_tables['Model_Type'] == result_type)], this_path, strategies_both, layer=layer, r_type=f'both-{result_type}')\n",
    "\n",
    "        #plot_bar_graphs_special(baseline_tables, result_tables, os.path.join(combined_path, 'strats'), strategies)\n",
    "        #plot_bar_graphs(baseline_tables[(baseline_tables['Model_Type'] == 'mlp1')], result_tables[(result_tables['Model_Type'] == 'mlp1')], os.path.join(combined_path, f'strats-rt-{retrain}'), strategies)\n",
    "\n",
    "    print('Original columns and epochs:', all_epochs_df.columns, all_epochs_df['epoch'].unique())\n",
    "\n",
    "    all_epochs_df = adjust_epochs(all_epochs_df)\n",
    "\n",
    "    grouped_df = group_eval_df(all_epochs_df)\n",
    "\n",
    "    print('merging dfs')\n",
    "    merged_df = pd.merge(combined_ifr_df, grouped_df, on=['rep', 'model', 'epoch', 'retrain', 'prior'])\n",
    "    print('after merge', merged_df['epoch'].unique(), merged_df['retrain'].unique(), merged_df['prior'].unique())\n",
    "\n",
    "    # SPLOM\n",
    "    for act in ['all_activations', 'final_layer_activations', 'input_activations']:\n",
    "        try:\n",
    "            make_splom(merged_df[(merged_df['act'] == act)], combined_path, act, False, True)\n",
    "            make_scatter(merged_df[merged_df['act'] == act], combined_path, act)\n",
    "        except BaseException as e:\n",
    "            print('failed a splom', e)\n",
    "\n",
    "    make_corr_things(merged_df, combined_path)\n",
    "\n",
    "    #mean_correlation_df = correlation_df.groupby('feature')['correlation'].mean().reset_index()\n",
    "\n",
    "    last_epoch_df = load_dataframes(last_path_list, key_param_list, key_param)\n",
    "\n",
    "    print('last epoch df columns', last_epoch_df.columns)\n",
    "\n",
    "    if all_epochs_df is not None and last_epoch_df is not None:\n",
    "        create_combined_histogram(last_epoch_df, all_epochs_df, key_param, os.path.join('supervised', exp_name))\n",
    "\n",
    "        avg_loss, variances, ranges_1, ranges_2, range_dict, range_dict3, stats, key_param_stats, oracle_stats, delta_sum, delta_x = calculate_statistics(\n",
    "            all_epochs_df, last_epoch_df, list(set(params + prior_metrics + [key_param])),\n",
    "            skip_3x=True, skip_1x=True, key_param=key_param, used_regimes=used_regimes, savepath=os.path.join('supervised', exp_name), last_timestep=True)\n",
    "\n",
    "\n",
    "\n",
    "    write_metrics_to_file(os.path.join(combined_path, 'metrics.txt'), last_epoch_df, ranges_1, params, stats,\n",
    "                          key_param=key_param, d_s=delta_sum, d_x=delta_x)\n",
    "    save_figures(combined_path, combined_ifr_df, avg_loss, ranges_2, range_dict, range_dict3,\n",
    "                 params, last_epoch_df, num=12, key_param_stats=key_param_stats, oracle_stats=oracle_stats,\n",
    "                 key_param=key_param, delta_sum=delta_sum, delta_x=delta_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae0d91e-9a74-4c7b-a781-12400eeea96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joel\\anaconda3\\envs\\torch\\lib\\site-packages\\supersuit\\__init__.py:20: UserWarning: You're using SuperSuit 3.0, released 7/7/21. The entire codebase has been rewritten or refactored as part of this release. While we've tested it thoroughly, please ensure everything you're doing still works properly and report any issues at https://github.com/PettingZoo-Team/SuperSuit. This warning will be removed 2 months after release.\n",
      "  warnings.warn(\"You're using SuperSuit 3.0, released 7/7/21. The entire codebase has been rewritten or refactored as part of this release. While we've tested it thoroughly, please ensure everything you're doing still works properly and report any issues at https://github.com/PettingZoo-Team/SuperSuit. This warning will be removed 2 months after release.\")\n",
      "c:\\users\\joel\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "import multiprocessing\n",
    "from itertools import product\n",
    "\n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from src.pz_envs import ScenarioConfigs\n",
    "from src.supervised_learning import gen_data\n",
    "from src.utils.plotting import create_combined_histogram, plot_progression, save_key_param_figures, plot_learning_curves, make_splom, make_ifrscores, make_scatter, make_corr_things, make_splom_aux, plot_strategy_bar, \\\n",
    "    create_faceted_heatmap, plot_bar_graphs, plot_bar_graphs_special, plot_bar_graphs_new, plot_dependency_bar_graphs, plot_dependency_bar_graphs_new\n",
    "from supervised_learning_main import run_supervised_session, calculate_statistics, write_metrics_to_file, save_figures, \\\n",
    "    train_model\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2303e3fa-a71f-41a7-aa8c-4a66708adcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def experiments(todo, repetitions, epochs=50, batches=5000, skip_train=False, skip_calc=False, batch_size=64, desired_evals=5,\n",
    "                skip_eval=False, skip_activations=False, last_timestep=True, retrain=False, current_model_type=None, current_label=None, current_label_name=None,\n",
    "                comparison=False):\n",
    "\n",
    "    save_every = max(1, epochs // desired_evals)\n",
    "\n",
    "    params = ['visible_baits', 'swaps', 'visible_swaps', 'first_swap_is_both',\n",
    "              'second_swap_to_first_loc', 'delay_2nd_bait', 'first_bait_size',\n",
    "              'uninformed_bait', 'uninformed_swap', 'first_swap', 'test_regime']\n",
    "    prior_metrics = ['shouldAvoidSmall', 'correct-loc', 'incorrect-loc',\n",
    "                     'shouldGetBig', 'informedness', 'p-b-0', 'p-b-1', 'p-s-0', 'p-s-1', 'delay', 'opponents']\n",
    "\n",
    "    sub_regime_keys = [\n",
    "        \"Nn\",\"Fn\", \"Nf\",\"Tn\", \"Nt\",\"Ff\",\"Tf\", \"Ft\",\"Tt\"\n",
    "    ]\n",
    "    all_regimes = ['sl-' + x + '0' for x in sub_regime_keys] + ['sl-' + x + '1' for x in sub_regime_keys]\n",
    "    mixed_regimes = {k: ['sl-' + x + '0' for x in sub_regime_keys] + ['sl-' + k + '1'] for k in sub_regime_keys}\n",
    "\n",
    "    regimes = {}\n",
    "    regimes['direct'] = ['sl-' + x + '1' for x in sub_regime_keys]\n",
    "    regimes['noOpponent'] = ['sl-' + x + '0' for x in sub_regime_keys]\n",
    "    regimes['everything'] = all_regimes\n",
    "    hregime = {}\n",
    "    hregime['homogeneous'] = ['sl-Tt0', 'sl-Ff0', 'sl-Nn0', 'sl-Tt1', 'sl-Ff1', 'sl-Nn1']\n",
    "    #hregime['identity'] = ['sl-' + x + '0' for x in sub_regime_keys] + ['sl-Tt1', 'sl-Ff1', 'sl-Nn1']\n",
    "    sregime = {}\n",
    "    sregime['special'] = ['sl-Tt0', 'sl-Tt1', 'sl-Nt0', 'sl-Nt1', 'sl-Nf0', 'sl-Nf1', 'sl-Nn0', 'sl-Nn1']\n",
    "\n",
    "    fregimes = {}\n",
    "    fregimes['s1'] = regimes['noOpponent']\n",
    "    fregimes['s3'] = regimes['everything']\n",
    "    fregimes['s2'] = mixed_regimes['Tt']\n",
    "    #fregimes['homogeneous'] = hregime['homogeneous']\n",
    "\n",
    "    single_regimes = {k[3:]: [k] for k in all_regimes}\n",
    "    leave_one_out_regimes = {}\n",
    "    for i in range(len(sub_regime_keys)):\n",
    "        regime_name = \"lo_\" + sub_regime_keys[i]\n",
    "        leave_one_out_regimes[regime_name] = ['sl-' + x + '0' for x in sub_regime_keys]\n",
    "        ones = ['sl-' + x + '1' for j, x in enumerate(sub_regime_keys) if j != i]\n",
    "        leave_one_out_regimes[regime_name].extend(ones)\n",
    "\n",
    "    pref_types = [\n",
    "        ('same', ''), # ('different', 'd'), # ('varying', 'v'),\n",
    "    ]\n",
    "    role_types = [\n",
    "        ('subordinate', ''), # ('dominant', 'D'), # ('varying', 'V'),\n",
    "    ]\n",
    "\n",
    "    # labels for ICLR, including size just in case\n",
    "\n",
    "    model_type = \"loc\" # or box\n",
    "    labels = [\n",
    "        'id', 'i-informedness', # must have these or it will break\n",
    "        'opponents',\n",
    "        'big-loc',\n",
    "        'small-loc',\n",
    "        'target-loc',\n",
    "        'b-loc',\n",
    "        'fb-loc',\n",
    "        'fb-exist',\n",
    "        'vision',\n",
    "        'big-box',\n",
    "        'small-box',\n",
    "        'target-box',\n",
    "        'b-box',\n",
    "        'fb-box',\n",
    "        'box-locations'\n",
    "              ]\n",
    "\n",
    "\n",
    "    oracles = labels + [None]\n",
    "    conf = ScenarioConfigs()\n",
    "    exp_name = f'exp_{todo[0]}'\n",
    "    if last_timestep:\n",
    "        exp_name += \"-L\"\n",
    "\n",
    "    session_params = {\n",
    "        'repetitions': repetitions,\n",
    "        'epochs': epochs,\n",
    "        'batches': batches,\n",
    "        'skip_train': skip_train,\n",
    "        'skip_eval': skip_eval,\n",
    "        'batch_size': batch_size,\n",
    "        'prior_metrics': list(set(prior_metrics + labels)),\n",
    "        'save_every': save_every,\n",
    "        'skip_calc': skip_calc,\n",
    "        'act_label_names': labels,\n",
    "        'skip_activations': skip_activations,\n",
    "        #'oracle_is_target': False,\n",
    "        'last_timestep': last_timestep,\n",
    "    }\n",
    "    if 0 in todo:\n",
    "        print('Generating datasets with labels', labels)\n",
    "        os.makedirs('supervised', exist_ok=True)\n",
    "        for pref_type, pref_suffix in pref_types:\n",
    "            for role_type, role_suffix in role_types:\n",
    "                gen_data(labels, path='supervised', pref_type=pref_suffix, role_type=role_suffix,\n",
    "                         prior_metrics=prior_metrics, conf=conf)\n",
    "\n",
    "    if 'h' in todo:\n",
    "        print('Running hyperparameter search on all regimes, pref_types, role_types')\n",
    "        run_hparam_search(trials=100, repetitions=3, log_file='hparam_file.txt', train_sets=regimes['direct'], epochs=20)\n",
    "\n",
    "    if 2 in todo:\n",
    "        print('Running experiment 1: base, different models and answers')\n",
    "\n",
    "        combined_path_list = []\n",
    "        last_path_list = []\n",
    "        lp_list = []\n",
    "        key_param = 'regime'\n",
    "        key_param_list = []\n",
    "        session_params['oracle_is_target'] = False\n",
    "\n",
    "        if current_model_type != None:\n",
    "            model_types = [current_model_type]\n",
    "            label_tuples = [(current_label, current_label_name)]\n",
    "        else:\n",
    "            model_types = ['cnn', 'smlp', 'clstm']\n",
    "            label_tuples = [('correct-loc', 'loc')]\n",
    "\n",
    "        for label, label_name in label_tuples: #[('correct-loc', 'loc'), ('correct-box', 'box'), ('shouldGetBig', 'size')]:\n",
    "            for model_type in model_types:#['smlp', 'cnn', 'clstm', ]:\n",
    "                for regime in list(fregimes.keys()):\n",
    "                    kpname = f'{model_type}-{label_name}-{regime}'\n",
    "                    print(model_type + '-' + label_name, 'regime:', regime, 'train_sets:', fregimes[regime])\n",
    "                    combined_paths, last_epoch_paths, lp = run_supervised_session(\n",
    "                        save_path=os.path.join('supervised', exp_name, kpname),\n",
    "                        train_sets=fregimes[regime],\n",
    "                        eval_sets=fregimes['s3'],\n",
    "                        oracle_labels=[None],\n",
    "                        key_param=key_param,\n",
    "                        key_param_value=kpname,\n",
    "                        label=label,\n",
    "                        model_type=model_type,\n",
    "                        do_retrain_model=retrain,\n",
    "                        **session_params\n",
    "                    )\n",
    "                    conditions = [\n",
    "                        (lambda x: 'prior' not in x and 'retrain' not in x, ''),\n",
    "                        #(lambda x: 'prior' in x and 'retrain' not in x, '-prior'),\n",
    "                        #(lambda x: 'prior' not in x and 'retrain' in x, '-retrain')\n",
    "                    ]\n",
    "\n",
    "                    print('paths found', combined_paths, last_epoch_paths)\n",
    "\n",
    "                    for condition, suffix in conditions:\n",
    "                        last_path_list.append([x for x in last_epoch_paths if condition(x)])\n",
    "                        combined_path_list.append([x for x in combined_paths if condition(x)])\n",
    "                        key_param_list.append(kpname + suffix)\n",
    "                    lp_list.append(lp) # has x, x-retrain currently\n",
    "\n",
    "        if comparison:\n",
    "            do_comparison(combined_path_list, last_path_list, key_param_list, key_param, exp_name, params, prior_metrics, lp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5c3f310-4b45-416d-8c38-551a4dbb477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nn': 24, 'Tt': 104, 'Nt': 32, 'Tn': 32, 'Ff': 28, 'Tf': 16, 'Ft': 16, 'Nf': 22, 'Fn': 22}\n",
      "list_events 296 total fillers 440 total permutations 23520\n",
      "Running experiment 1: base, different models and answers\n",
      "cnn-loc regime: s1 train_sets: ['sl-Nn0', 'sl-Fn0', 'sl-Nf0', 'sl-Tn0', 'sl-Nt0', 'sl-Ff0', 'sl-Tf0', 'sl-Ft0', 'sl-Tt0']\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\cnn-loc-s1\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1-retrain\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\cnn-loc-s1\n",
      "paths found ['supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1-retrain\\\\param_losses_49_2.csv'] ['supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s1-retrain\\\\param_losses_49_2.csv']\n",
      "cnn-loc regime: s3 train_sets: ['sl-Nn0', 'sl-Fn0', 'sl-Nf0', 'sl-Tn0', 'sl-Nt0', 'sl-Ff0', 'sl-Tf0', 'sl-Ft0', 'sl-Tt0', 'sl-Nn1', 'sl-Fn1', 'sl-Nf1', 'sl-Tn1', 'sl-Nt1', 'sl-Ff1', 'sl-Tf1', 'sl-Ft1', 'sl-Tt1']\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\cnn-loc-s3\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3-retrain\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\cnn-loc-s3\n",
      "paths found ['supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3-retrain\\\\param_losses_49_2.csv'] ['supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s3-retrain\\\\param_losses_49_2.csv']\n",
      "cnn-loc regime: s2 train_sets: ['sl-Nn0', 'sl-Fn0', 'sl-Nf0', 'sl-Tn0', 'sl-Nt0', 'sl-Ff0', 'sl-Tf0', 'sl-Ft0', 'sl-Tt0', 'sl-Tt1']\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\cnn-loc-s2\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2-retrain\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\cnn-loc-s2\n",
      "paths found ['supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2-retrain\\\\param_losses_49_2.csv'] ['supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\cnn-loc-s2-retrain\\\\param_losses_49_2.csv']\n",
      "smlp-loc regime: s1 train_sets: ['sl-Nn0', 'sl-Fn0', 'sl-Nf0', 'sl-Tn0', 'sl-Nt0', 'sl-Ff0', 'sl-Tf0', 'sl-Ft0', 'sl-Tt0']\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\smlp-loc-s1\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1-retrain\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\smlp-loc-s1\n",
      "paths found ['supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1-retrain\\\\param_losses_49_2.csv'] ['supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s1-retrain\\\\param_losses_49_2.csv']\n",
      "smlp-loc regime: s3 train_sets: ['sl-Nn0', 'sl-Fn0', 'sl-Nf0', 'sl-Tn0', 'sl-Nt0', 'sl-Ff0', 'sl-Tf0', 'sl-Ft0', 'sl-Tt0', 'sl-Nn1', 'sl-Fn1', 'sl-Nf1', 'sl-Tn1', 'sl-Nt1', 'sl-Ff1', 'sl-Tf1', 'sl-Ft1', 'sl-Tt1']\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\smlp-loc-s3\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3-retrain\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\smlp-loc-s3\n",
      "paths found ['supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3-retrain\\\\param_losses_49_2.csv'] ['supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s3-retrain\\\\param_losses_49_2.csv']\n",
      "smlp-loc regime: s2 train_sets: ['sl-Nn0', 'sl-Fn0', 'sl-Nf0', 'sl-Tn0', 'sl-Nt0', 'sl-Ff0', 'sl-Tf0', 'sl-Ft0', 'sl-Tt0', 'sl-Tt1']\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\smlp-loc-s2\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2-retrain\\\\param_losses_49_2.csv'] supervised\\exp_2-L\\smlp-loc-s2\n",
      "paths found ['supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2-retrain\\\\param_losses_49_2.csv'] ['supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\smlp-loc-s2-retrain\\\\param_losses_49_2.csv']\n",
      "clstm-loc regime: s1 train_sets: ['sl-Nn0', 'sl-Fn0', 'sl-Nf0', 'sl-Tn0', 'sl-Nt0', 'sl-Ff0', 'sl-Tf0', 'sl-Ft0', 'sl-Tt0']\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_9_2.csv'] supervised\\exp_2-L\\clstm-loc-s1\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_9_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_9_2.csv'] supervised\\exp_2-L\\clstm-loc-s1\n",
      "paths found ['supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_9_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_9_2.csv'] ['supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s1-retrain\\\\param_losses_49_2.csv']\n",
      "clstm-loc regime: s3 train_sets: ['sl-Nn0', 'sl-Fn0', 'sl-Nf0', 'sl-Tn0', 'sl-Nt0', 'sl-Ff0', 'sl-Tf0', 'sl-Ft0', 'sl-Tt0', 'sl-Nn1', 'sl-Fn1', 'sl-Nf1', 'sl-Tn1', 'sl-Nt1', 'sl-Ff1', 'sl-Tf1', 'sl-Ft1', 'sl-Tt1']\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_9_2.csv'] supervised\\exp_2-L\\clstm-loc-s3\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_9_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_9_2.csv'] supervised\\exp_2-L\\clstm-loc-s3\n",
      "paths found ['supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_9_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_9_2.csv'] ['supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s3-retrain\\\\param_losses_49_2.csv']\n",
      "clstm-loc regime: s2 train_sets: ['sl-Nn0', 'sl-Fn0', 'sl-Nf0', 'sl-Tn0', 'sl-Nt0', 'sl-Ff0', 'sl-Tf0', 'sl-Ft0', 'sl-Tt0', 'sl-Tt1']\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_9_2.csv'] supervised\\exp_2-L\\clstm-loc-s2\n",
      "sup sess dfs paths ['supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_9_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_9_2.csv'] supervised\\exp_2-L\\clstm-loc-s2\n",
      "paths found ['supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_9_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_9_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_9_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_9_2.csv'] ['supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2\\\\param_losses_49_2.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_49_0.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_49_1.csv', 'supervised\\\\exp_2-L\\\\clstm-loc-s2-retrain\\\\param_losses_49_2.csv']\n",
      "loading IFRs\n",
      "duplicates removed 10176 10176\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "experiments([2], repetitions=3,\n",
    "                batches=10000,\n",
    "                skip_train=True,\n",
    "                skip_eval=True,\n",
    "                skip_calc=True,\n",
    "                skip_activations=True,\n",
    "                retrain=False,\n",
    "                batch_size=256,\n",
    "                desired_evals=1,\n",
    "                last_timestep=True,\n",
    "                comparison=True)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17081a4c-2ea7-40e5-be50-8f262271ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sort_key(model):\n",
    "    if 'mlp' in model.lower():\n",
    "        return 1\n",
    "    elif 'cnn' in model.lower():\n",
    "        return 2\n",
    "    elif 'lstm' in model.lower():\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42d7ac0b-5f41-4b4e-bccf-18d66ea305a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_graphs_new(baselines_df, results_df, save_dir, strategies, one_dataset=True, one_model=False, layer='all', r_type='mlp1'):\n",
    "    model_classes = sorted(set([model[:-7] for model in results_df['Model'].unique()]), key=get_sort_key)\n",
    "    datasets = sorted(set([model[-2:] for model in results_df['Model'].unique()]))\n",
    "    all_features = sorted(set([feature for features in strategies.values() for feature in features]))\n",
    "\n",
    "    model_name_map = {\n",
    "        'smlp': 'MLP Model',\n",
    "        'cnn': 'CNN Model',\n",
    "        'clstm': 'CLSTM Model'\n",
    "    }\n",
    "    \n",
    "    model_colors = {\n",
    "        'Raw Input': '#808080',\n",
    "        'smlp': '#FF4444',\n",
    "        'cnn': '#9944FF',\n",
    "        'clstm': '#4444FF'\n",
    "    }\n",
    "    \n",
    "    strategy_colors = {\n",
    "        'No-Mindreading': '#FFE5E5',\n",
    "        'Low-Mindreading': '#FFFAE5',\n",
    "        'High-Mindreading': '#E5F0FF'\n",
    "    }\n",
    "    \n",
    "    dataset_name_map = {\n",
    "        's1': 'Stage 1 Training',\n",
    "        's2': 'Stage 2 Training',\n",
    "        's3': 'Stage 3 Training'\n",
    "    }\n",
    "\n",
    "    fig = plt.figure(figsize=(8 * len(datasets), 5.5 * (len(model_classes) + 1) + 2))\n",
    "    \n",
    "    gs = fig.add_gridspec(len(model_classes) + 2, len(datasets), \n",
    "                         height_ratios=[4]*(len(model_classes) + 1) + [1],\n",
    "                         hspace=0.45,\n",
    "                         wspace=0.05)\n",
    "    axs = [[fig.add_subplot(gs[i, j]) for j in range(len(datasets))] for i in range(len(model_classes) + 1)]\n",
    "    \n",
    "    bar_width = 0.15\n",
    "    feature_gap = 0.03\n",
    "    strategy_gap = 0.25 if 'spe' not in r_type else 0.15\n",
    "    error_offset = bar_width * 0.25\n",
    "\n",
    "    # Calculate total width for each strategy group\n",
    "    strategy_widths = {}\n",
    "    for strategy, features in strategies.items():\n",
    "        total_bars = len(features)\n",
    "        strategy_widths[strategy] = total_bars * bar_width + (total_bars - 1) * feature_gap + strategy_gap\n",
    "\n",
    "    # Add strategy backgrounds\n",
    "    for col in range(len(datasets)):\n",
    "        ax = axs[0][col]\n",
    "        x_offset = -bar_width\n",
    "        for strategy, features in strategies.items():\n",
    "            width = strategy_widths[strategy]\n",
    "            rect = plt.Rectangle((x_offset, 0), width, 1,\n",
    "                               facecolor=strategy_colors[strategy],\n",
    "                               transform=ax.get_xaxis_transform(),\n",
    "                               zorder=-1,\n",
    "                               clip_on=False)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            for row in range(1, len(model_classes) + 1):\n",
    "                rect = plt.Rectangle((x_offset, 0), width, 1,\n",
    "                                   facecolor=strategy_colors[strategy],\n",
    "                                   transform=axs[row][col].get_xaxis_transform(),\n",
    "                                   zorder=-1,\n",
    "                                   clip_on=False)\n",
    "                axs[row][col].add_patch(rect)\n",
    "            x_offset += width\n",
    "\n",
    "    # Plot baselines row (row 0)\n",
    "    for col, dataset in enumerate(datasets):\n",
    "        ax = axs[0][col]\n",
    "        x_offset = 0\n",
    "        \n",
    "        for strategy_idx, (strategy, features) in enumerate(strategies.items()):\n",
    "            for feature_idx, feature in enumerate(features):\n",
    "                baseline = baselines_df[(baselines_df['Feature'] == feature) & \n",
    "                                     (baselines_df['Model'] == dataset)]\n",
    "                \n",
    "                if len(baseline) > 0:\n",
    "                    baseline_familiar = baseline['Familiar accuracy (input-activations)'].values[0]\n",
    "                    baseline_novel = baseline['Novel accuracy (input-activations)'].values[0]\n",
    "                    baseline_familiar_err = [baseline['Familiar q1 (input-activations)'].values[0],\n",
    "                                          baseline['Familiar q3 (input-activations)'].values[0]]\n",
    "                    baseline_novel_err = [baseline['Novel q1 (input-activations)'].values[0],\n",
    "                                        baseline['Novel q3 (input-activations)'].values[0]]\n",
    "                    \n",
    "                    # Draw bars without error bars\n",
    "                    ax.bar(x_offset, baseline_familiar, bar_width,\n",
    "                          color='white', edgecolor=model_colors['Raw Input'])\n",
    "                    ax.bar(x_offset, baseline_novel, bar_width,\n",
    "                          color=model_colors['Raw Input'])\n",
    "                    \n",
    "                    # Add error bars separately with offset\n",
    "                    ax.errorbar(x_offset + error_offset, baseline_familiar,\n",
    "                              yerr=[[baseline_familiar - baseline_familiar_err[0]],\n",
    "                                   [baseline_familiar_err[1] - baseline_familiar]],\n",
    "                              color='black', capsize=3, fmt='none')\n",
    "                    ax.errorbar(x_offset - error_offset, baseline_novel,\n",
    "                              yerr=[[baseline_novel - baseline_novel_err[0]],\n",
    "                                   [baseline_novel_err[1] - baseline_novel]],\n",
    "                              color='black', capsize=3, fmt='none')\n",
    "                    \n",
    "                    ax.text(x_offset, -0.05, feature, ha='right', va='top',\n",
    "                           rotation=45, rotation_mode='anchor', fontsize=12)\n",
    "                \n",
    "                x_offset += bar_width + feature_gap\n",
    "            x_offset += strategy_gap\n",
    "\n",
    "        ax.set_ylabel('Accuracy' if col == 0 else '')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_xlim(-bar_width, x_offset - strategy_gap)\n",
    "        ax.set_xticks([])\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        \n",
    "        if col == 0:\n",
    "            ax.text(-0.25, 0.5, 'Raw Input', va='center', ha='right', \n",
    "                   color=model_colors['Raw Input'], fontsize=24, fontweight='bold')\n",
    "        \n",
    "        if col != 0:\n",
    "            ax.yaxis.set_ticks([])\n",
    "            \n",
    "        ax.set_title(dataset_name_map[dataset], fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot model results (rows 1 onwards)\n",
    "    for row, model_class in enumerate(model_classes, start=1):\n",
    "        for col, dataset in enumerate(datasets):\n",
    "            ax = axs[row][col]\n",
    "            x_offset = 0\n",
    "            \n",
    "            for strategy_idx, (strategy, features) in enumerate(strategies.items()):\n",
    "                for feature_idx, feature in enumerate(features):\n",
    "                    model_name = f\"{model_class}-loc-{dataset}\"\n",
    "                    results = results_df[(results_df['Feature'] == feature) & \n",
    "                                      (results_df['Model'] == model_name)]\n",
    "                    \n",
    "                    if len(results) > 0:\n",
    "                        result_familiar = results[f'Familiar accuracy ({layer}-activations)'].values[0]\n",
    "                        result_novel = results[f'Novel accuracy ({layer}-activations)'].values[0]\n",
    "                        result_familiar_err = [results[f'Familiar q1 ({layer}-activations)'].values[0],\n",
    "                                            results[f'Familiar q3 ({layer}-activations)'].values[0]]\n",
    "                        result_novel_err = [results[f'Novel q1 ({layer}-activations)'].values[0],\n",
    "                                          results[f'Novel q3 ({layer}-activations)'].values[0]]\n",
    "\n",
    "                        # Draw bars without error bars\n",
    "                        ax.bar(x_offset, result_familiar, bar_width,\n",
    "                              color='white', edgecolor=model_colors[model_class])\n",
    "                        ax.bar(x_offset, result_novel, bar_width,\n",
    "                              color=model_colors[model_class])\n",
    "                        \n",
    "                        # Add error bars separately with offset\n",
    "                        ax.errorbar(x_offset + error_offset, result_familiar,\n",
    "                                  yerr=[[result_familiar - result_familiar_err[0]],\n",
    "                                       [result_familiar_err[1] - result_familiar]],\n",
    "                                  color=model_colors[model_class], capsize=3, fmt='none')\n",
    "                        ax.errorbar(x_offset - error_offset, result_novel,\n",
    "                                  yerr=[[result_novel - result_novel_err[0]],\n",
    "                                       [result_novel_err[1] - result_novel]],\n",
    "                                  color=model_colors[model_class], capsize=3, fmt='none')\n",
    "                        \n",
    "                        ax.text(x_offset, -0.05, feature, ha='right', va='top',\n",
    "                               rotation=45, rotation_mode='anchor', fontsize=12)\n",
    "                    \n",
    "                    x_offset += bar_width + feature_gap\n",
    "                x_offset += strategy_gap\n",
    "\n",
    "            ax.set_ylabel('Accuracy' if col == 0 else '')\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xlim(-bar_width, x_offset - strategy_gap)\n",
    "            ax.set_xticks([])\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "\n",
    "            if col != 0:\n",
    "                ax.yaxis.set_ticks([])\n",
    "\n",
    "            if col == 0:\n",
    "                ax.text(-0.25, 0.5, model_name_map[model_class], va='center', ha='right', \n",
    "                       color=model_colors[model_class], fontsize=24, fontweight='bold')\n",
    "\n",
    "    # Add strategy labels at bottom\n",
    "    for col in range(len(datasets)):\n",
    "        ax = axs[-1][col]\n",
    "        x_offset = -bar_width\n",
    "        for strategy, features in strategies.items():\n",
    "            width = strategy_widths[strategy]\n",
    "            center = x_offset + width/2\n",
    "            strategy_text = strategy.replace('-', '\\n')\n",
    "            ax.text(center, -0.3, strategy_text, ha='center', va='top', \n",
    "                   fontsize=16, fontweight='bold')\n",
    "            x_offset += width\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(f'{save_dir}/{layer}_models_datasets_accuracy_comparison_{r_type}.png', \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9adedbea-b400-4a64-aded-c2f8e1334a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made acc tables 54 288\n",
      "Index(['Feature', 'Model', 'Model_Type',\n",
      "       'Familiar accuracy (input-activations)',\n",
      "       'Familiar between-model std (input-activations)',\n",
      "       'Familiar within-model std (input-activations)',\n",
      "       'Familiar q1 (input-activations)', 'Familiar q3 (input-activations)',\n",
      "       'Novel accuracy (input-activations)',\n",
      "       'Novel between-model std (input-activations)',\n",
      "       'Novel within-model std (input-activations)',\n",
      "       'Novel q1 (input-activations)', 'Novel q3 (input-activations)',\n",
      "       'data_Type'],\n",
      "      dtype='object') Index(['Feature', 'Model', 'Model_Type', 'Familiar accuracy (all-activations)',\n",
      "       'Familiar between-model std (all-activations)',\n",
      "       'Familiar within-model std (all-activations)',\n",
      "       'Familiar q1 (all-activations)', 'Familiar q3 (all-activations)',\n",
      "       'Novel accuracy (all-activations)',\n",
      "       'Novel between-model std (all-activations)',\n",
      "       'Novel within-model std (all-activations)',\n",
      "       'Novel q1 (all-activations)', 'Novel q3 (all-activations)',\n",
      "       'Familiar accuracy (final-layer-activations)',\n",
      "       'Familiar between-model std (final-layer-activations)',\n",
      "       'Familiar within-model std (final-layer-activations)',\n",
      "       'Familiar q1 (final-layer-activations)',\n",
      "       'Familiar q3 (final-layer-activations)',\n",
      "       'Novel accuracy (final-layer-activations)',\n",
      "       'Novel between-model std (final-layer-activations)',\n",
      "       'Novel within-model std (final-layer-activations)',\n",
      "       'Novel q1 (final-layer-activations)',\n",
      "       'Novel q3 (final-layer-activations)', 'data_Type'],\n",
      "      dtype='object')\n",
      "doing\n",
      "doing\n",
      "doing\n",
      "doing\n"
     ]
    }
   ],
   "source": [
    "last_timestep = True\n",
    "exp_name = f'exp_{2}'\n",
    "if last_timestep:\n",
    "    exp_name += \"-L\"\n",
    "\n",
    "combined_path = os.path.join('supervised', exp_name, 'c')\n",
    "\n",
    "strategies_short = {\n",
    "    'No-Mindreading': ['opponents'],\n",
    "    'High-Mindreading': ['b-loc']\n",
    "}\n",
    "strategies_long = {\n",
    "    'No-Mindreading': ['pred', 'opponents', 'big-loc', 'small-loc'],\n",
    "    'Low-Mindreading': ['vision', 'fb-exist'],\n",
    "    'High-Mindreading': ['fb-loc', 'b-loc', 'target-loc', 'labels']\n",
    "}\n",
    "strategies_both = {\n",
    "    'No-Mindreading': ['big-loc', 'big-box', 'small-loc', 'small-box'],\n",
    "    'High-Mindreading': ['fb-loc', 'fb-box', 'b-loc', 'b-box', 'target-loc', 'target-box', ]\n",
    "}\n",
    "\n",
    "#for retrain in [True, False]:\n",
    "for retrain in [False]:\n",
    "    baseline_tables = pd.read_csv(os.path.join(combined_path, f'base_all_table_retrain_False.csv'))\n",
    "    result_tables = pd.read_csv(os.path.join(combined_path, f'all_table_retrain_{retrain}.csv'))\n",
    "\n",
    "    print('made acc tables', len(baseline_tables), len(result_tables))\n",
    "    print(baseline_tables.columns, result_tables.columns)\n",
    "\n",
    "    this_path = os.path.join(combined_path, f'strats-rt-{retrain}')\n",
    "\n",
    "    for result_type in ['mlp1', 'linear']:\n",
    "        for layer in ['all', 'final-layer']:\n",
    "            print('doing')\n",
    "            plot_bar_graphs_new2(baseline_tables[(baseline_tables['Model_Type'] == result_type)], result_tables[(result_tables['Model_Type'] == result_type)], this_path, strategies_short, layer=layer, r_type=f'spec-{result_type}')\n",
    "            plot_bar_graphs_new2(baseline_tables[(baseline_tables['Model_Type'] == result_type)], result_tables[(result_tables['Model_Type'] == result_type)], this_path, strategies_long, layer=layer, r_type=result_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66102b-ec65-4ba7-ae64-7f01a89589d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
